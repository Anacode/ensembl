 - bioperl modules/


Pipeline documentation
----------------------

This document contains step by step instructions for installing and running the Ensembl analysis pipeline.

if you find any omissions, errors, or just confusing bits in the instructions please send an email to helpdesk@ensembl.org and we'll try to fix things.

A word of warning.  This installation is not for the faint hearted and will take more than an afternoon's work to get running.  But it will run.  Eventually.  Stick with it and you will have the very satisfied feeling of
having an analysis pipeline gently purring away doing jolly useful jobs.

Good luck.

Michele Clamp August 2001

Introduction
------------

The Ensembl analysis pipeline is split into 3 parts 

    - the core ensembl database (i.e. the one the web site runs off containing the genes,dna and features), 
    - the job tracking database and code
    - the code and binaries that actually run analyses

Each of these three parts is independent of each other.  i.e. the core ensembl database can run on its own (obviously) without any need for the other two parts. The job tracking database doesn't care what sort of database is underneath it or what sort of analysis it's running and the analysis modules can be used standalone without any ensembl database or tracking database.  This last property makes code very easy to develop and most importantly *test* as I would not recommend trying to debug a set of perl modules through LSF and a remote compute farm.

Having said all that this document assumes that you will be using an ensembl database, the tracking database and the ensembl analysis code all together.

Instruction Summary
-------------------

  - Prerequisites.  i.e. what your system has to have installed before you start running anything.
  - Binary and data installation
  - Testing the code
  - Making an initial database
  - Pipeline configuration
  - The Tracking tables
  - Adding analysis processes to the tracking tables
  - Reading in sequence data
  - Testing individual analysisprocesses
  - Making rules
  - Deleting rules
  - The RuleManager (finally running the d*mn thing)
  - Runnables/RunnableDBs
  - Overview of the pipeline
  - Writing your own runnables
  - Using a different queueing system

Appendices
----------
 - sample .cshrc file
 - sample analysisprocess table
 - useful bits of sql.

Installing
----------

Prerequisites
-------------

-   Some sort of unix like system.  It is possible this may work on Mac OSX but it hasn't been tried. (Volunteers welcome)

-   If you are running a pipeline on the full Ensembl database you need about 12Gb of disk.  As a rule of
    thumb you will need 4 times the amount of sequence in disk space.  e.g. If you have 100Mb of dna to analyse
    you will need 400Mbytes of disk space.

-   cvs    (ftp.cvshome.org/LATEST)  This is needed to download the Ensembl source code.  Most unix
                                     systems have this but if not you can install it as follows

    Download the latest source and unpack into a working directory
       gunzip < cvs-1.11.1p1.tar.gz |tar xvf -

    cd cvs-1.11.1p1

    ./configure

    make

    make install

-   perl   (www.cpan.org)  You need perl version 5.004_04 and preferably 5.6.  To find out what
                           version of perl you have type perl -v

    mysql  (www.mysql.com/downloads) This is the database you will store your dna , job tracking data and analysis results in.


    Start up the mysql server

          /data/mysql/bin/safe_mysqld --user=mysqldba &

    To check everything is running bring up a mysql console

         /data/mysql/bin/mysql --user=mysqdba

    You will get a mysql prompt if successful

    mysql> 

    Perl modules

    Quite a few perl modules need to be installed.  These can all be downloaded from www.cpan.org and are all 
    installed in much the same way

    1. download the module tarball

    2. Unpack the tarball in a working directory with
       gunzip < tarball.tar.gz |tar xvf -

    3. Change into the tarball directory

    4. perl Makefile.PL

    5. make
   
    6. make install

    (You may have to be root to install.  If you want to install it in your own user space ^^^)

    The modules that are required are listed below, along with their URLs.  The file part of the URL
    is current at the time of writing - you should install whatever is the latest version.

    
    DBI	       - common database interface for perl (lets perl talk to mysql)
	   http://www.cpan.org/modules/by-module/DBI/DBI-1.16.tar.gz

    DBD::Mysql - the mysql drivers for the DBI module
	   http://www.cpan.org/modules/by-module/DBD/Msql-Mysql-modules-1.2216.tar.gz


    LSF    (or other queueing system) Not essential but necessary if you want to run things
           on more than one processor.  Initially you will be setting up the pipeline and running
           it locally on one processor.  Makes debugging less painful :-)

Installing the Ensembl,Pipeline and BioPerl modules
---------------------------------------------------

      Decide where the ensembl code is going to live - I will assume it's /usr/local/ensembl/src

      1. cd /usr/local/ensembl/src

      2. cvs -d :pserver:cvsuser@cvs.sanger.ac.uk:/cvsroot/CVSmaster login

      when prompted for a password use

      CVSUSER

      3. Type the following as a single command

         cvs -d :pserver:cvsuser@cvs.sanger.ac.uk:/cvsroot/CVSmaster co -r branch-ensembl-110 ensembl
         cvs -d :pserver:cvsuser@cvs.sanger.ac.uk:/cvsroot/CVSmaster co -r branch-ensembl-110 ensembl-pipeline
	 

     to install the bioperl modules

      1. cd /usr/local/ensembl/src

      2. cvs -d :pserver:cvs@cvs.bioperl.org:/home/repository/bioperl login

      then enter the password

      cvs

      3. Type the following as a single command

         cvs -d :pserver:cvs@cvs.bioperl.org:/home/repository/bioperl co -r branch-07 bioperl-live


You should now have all the ensembl perl code you need.


Analysis binary installation
----------------------------

Obviously a pipeline is not much use without anything to run through it.  Below is a list of the programs
that we run automatically as soon as data comes in.  Some programs are free and some need a license.  
This is the part of the installation that is most prone to breaking.  We are putting a whole load
of separate programs together that have been written by lots of different people and we have to trust that
their input and output formats are not going to change from under our feet and bring the whole thing
down around our ears.  This is why we have 2 separate blocks of tests for testing the analyses will
run before we even start *thinking* about tracking them through a database.


The ensembl pipeline has two different places for storing binaries and data needed to run the binaries.
All executables are put in one
directory and all databases, matrices etc are put in another directory.  You need to decide where you
are going to put things and make the relevant directories.  For instance

   /usr/local/ensembl/bin   (stores binaries  - blastp, genscan etc)
   /usr/local/ensembl/data  (stores databases - HumanIso.smat, swissprot, UniSTS etc)

   Make these directories now.  Don't try and reuse your old bioinformatics bin directory that you've 
   been storing binaries in for years (our one has 355 programs in it!).  Start afresh. Disk space
   is cheap and you will find it easier to see what is missing.

   I would also urge you to start a fresh set of blast databases but I realise this might not
   be feasible for lots of people so I'll let you make a link to your current set of blast databases.

    1. RepeatMasker (and libraries and friends e.g. cross_match)

       This is run on all incoming genomic dna to mask out common repeats.

       Download
          You need to contact Arian Smit directly at asmit@nootka.mbt.washington.edu . Commercial
          people need to contact Chuck Williams  at swxfr@u.washington.edu . Make sure you get
          the RepeatMasker scripts, the libraries and cross_match.


       - put libraries in /usr/local/ensembl/data/RepeatMasker
       - put RepeatMasker and ProcessRepeats and cross_match in /usr/local/ensembl/bin

      You now need to edit the RepeatMasker script and change the $DIRECTORY path
      to /usr/local/ensembl/data/RepeatMasker

      You also need to set the $processrepeats variable to /usr/local/ensembl/bin/ProcessRepeat
      Finally set the $crossmatch variable to /usr/local/ensembl/bin/cross_match


    2. e-PCR  (and libraries) 

        Places markers on genomic sequence

        1. Download

	  ftp://ftp.ncbi.nlm.nih.gov/pub/schuler/e-PCR/e-PCR.tar.Z

	  uncompress e-PCR.tar.Z
          mkdir e-PCR
          cd e-PCR
          tar xvf ../e-PCR.tar
	  cd src
	  make

          (If make fails complaining about can't find CC - change it to cxx)

          cp e-PCR /usr/local/ensembl/bin

       You also need a marker database to search. e-PCR comes with a couple in the db
       directory but a larger set is at

          ftp://ncbi.nlm.nih.gov/repository/dbSTS/UniSTS.sts
       or ftp://ncbi.nlm.nih.gov/repository/dbSTS/UniSTS_human.sts

       Whichever file you want and put it in the /usr/local/ensembl/data directory.

       3. cpg (Looks for CpG islands in genomic sequence)

       this 

      4. genscan (and libraries)

      Go to the academic license site http://genes.mit.edu/license.html , fill in and submit.  You 
      will then be taken to the download page where there are binaries for
         Sun/Solaris
         Intel/Linux
         SGI/Irix
         Compaq/Tru64
         Intel/Solaris

      Clicking on one of these links will download a uuencoded file e.g. genscantru64.tar.uue

      Uncompress this as follows :

       uudecode genscantru64.tar.uue
       mkdir genscan
       cd genscan
       tar xvf ../genscantru64.tar

       Copy the genscan file into /usr/local/ensembl/bin
       Copy the *.smat files into /usr/local/ensembl/data

       blast   (and the databases you want to search)

       Downloading and installing blast
       Formatting databases.

       Again put all the binaries (blastp,blastn,blastx,tblastn,tblastx) into
       /usr/local/ensembl/bin

       Put the database creating binaries (pressdb, setdb) into /usr/local/ensembl/bin
       as some of the more complicated modules need these

       Make a directory off your data directory 

         mkdir /usr/local/ensembl/data/blastdb

       (you can make this a link to your current set of dbs if you really must

            ln -s /my/crufty/blastdbs /usr/local/ensembl/data/blastdb
       )

       Put all your blast databases in here and your blast matrices in
        
        /usr/local/ensembl/data/blastmat

      You now need to set two environment variables BLASTDB and BLASTMAT

      For csh,tcsh

         setenv BLASTDB /usr/local/ensembl/data/blastdb
         setenv BLASTMAT /usr/local/ensembl/data/blastmat

     For ksh,bash
         
         export BLASTDB=/usr/local/ensembl/data/blastdb
         export BLASTMAT=/usr/local/ensembl/data/blastmat

     Put these lines in your .cshrc or your .bashrc depending
     on which shell you're using.

      genewise (and all its gubbins)

          Download from ftp://ftp.sanger.ac.uk/pub/genewise

         Copy the genewise binary into /usr/local/ensembl/bin/genewise

         
      est_genome

          Download from ?

      tandem

      tRNAscan
      
      MSPcrunch

      getseqs

      makeindex

      halfwise?
      
      exonerate?
    
Once everything is installed your bin directory should look something like this

     bin/
     bin/genscan
     bin/RepeatMasker
     bin/blastn
     bin/blastx
     bin/tblastn
     bin/blastp
     bin/tblastx
     bin/cross_match
     bin/ProcessRepeats
     bin/est_genome
     bin/cpg
     bin/exonerate
     bin/e-PCR
     bin/genewise
     bin/getseqs
     bin/pressdb
     bin/makeindex
     bin/setdb
     bin/MSPcrunch

Your data directory looks a bit like

     Arabidopsis.smat
     HumanIso.smat
     Maize.smat
     UniSTS
     blastdb/
     blastmat/
     wisecfg/


Perl configuration
------------------

Before starting you need to tell perl where to find the ensembl
modules.  This is done by setting your PERL5LIB environment
variable.

If you have downloaded all the code into a directory 

/usr/local/ensembl/src

i.e. that directory has the ensembl,ensemlb-pipeline,bioperl-live
directories in it you would type the following


setenv PERL5LIB /usr/local/ensembl/src/ensembl/modules:/usr/local/ensembl/src/ensembl-pipeline/modules:/usr/local/ensembl/src/bioperl-live/:${PERL5LIB}

This line should be entered in your .cshrc (or the equivalent in your .bashrc or .profile)

Path configuration
------------------

Having lovingly set up all your binaries together in one directory your shell has to know
how to find them.  Modify your PATH variable as follows

setenv PATH /usr/local/ensembl/bin/:${PATH}


Testing the standalone analysis
-------------------------------

Before loading up a database we need to make sure that all
the standalone programs are working.  There are a whole
set of test scripts in the ensembl-pipeline code to do this.


cd ensembl-pipeline/modules

Type 
     ls -1 t/*.t 

to show what tests there are.  Notice that some tests end in
DB and some don't.  The ones with the DB end get all their data
from an ensembl database whereas the ones without read in their
data from files. 

The simplest tests are the ones with no database access.  Try 
these first.

e.g.

  perl t/cpg.t
  perl t/epcr.t
  perl t/repeatmasker.t
  perl t/crossmatch.t
  perl t/blast.t
  perl t/genscan.t
  perl t/tandem.t
  perl t/tRNA.t
  perl t/est2genome.t
  
Most of the problems are usually due to the perl modules not
finding the right binaries.

If the tests pass you will get a series of ok messages.

Once these are working you can try the tests that use 
an ensembl database as a source of data.  You need to 
tell perl where to find the mysql installation and this is done 
in a file t/EnsTestDB.conf .

An example conf file is in t/EnsTestDB.conf.example

Copy this to t/EnsTestDB.conf and edit it.  You will 
probably need to change at least the host and the
user parameters to reflect your own setup.

Once this is done you can test the database connection

perl t/EnsTestDB.t

Problems with the database connection are usually user permission
problems in mysql.  Make sure that the user you are connecting
as has all the correct permissions (create,select,insert,update,drop).
See the mysql manual for details on how to do this.  If you have
installed mysql from scratch the root user has all the right privileges.
I wouldn't recommend running the whole pipeline as root though.

Once you can connect to the database you can run the following database
tests

   perl t/cpgDB.t
   perl t/epcrDB.t
   perl t/genscanDB.t
   perl t/repeatmaskerDB.t
   perl t/tRNADB.t
   perl t/blastDB.t
   perl t/blastgenscanpepDB.t
   perl t/blastgenscandnaDB.t

If everything has worked perfectly well done.  I'm amazed.  You're almost there.


Setting up the database
-----------------------

We're now ready to load up the sql for the ensembl and tracking databases and start learning
about configuring a pipeline.

cd /usr/local/ensembl/src/

This is where you have checked out all the ensembl/biopperl modules

Create a database 

Load up the sql as follows

  mysql < ensembl/sql/tables.sql 
  mysql < ensembl-pipeline/sql/tables.sql

You have a shiny new 

Pipeline configuration
----------------------
 - configuration
     pipeConf
        bindir
        datadir
        host
        dbname
        user
        queue name

 - Database schema and explanation
     - what an analysis process is
     - what a rule is
     - what a job is
     - what jobstatus is
     - what InputIdAnalysis is

 - Loading some sequence data

     - from fasta files
     - from an ensembl database

     - For the pipeline to pick up sequence data from the database it has to have
       an entry in the InputIdAnalysis table


 - testing a RunnableDB
     - when adding new runnables (runnableDBs) it is *always* a good idea
       to test them fully on the command line before sending them off
       naked into the wilderness of a compute farm.

     - with and without the write option

 - Inserting analysis process rows

     - description of the anlaysis process table
     - explanation of the existing anlaysisprocess entries
     - how to add new processes (prerequisities are a RunnableDB)
     - Example of adding a new line for a new blast job

 - Adding rules

 - Deleting rules


Running
-------
 - Adding in sequence
     scripts to do this

 - running the pipeline locally

     How to start the RuleManager
     Various options on the RuleManager


 - Other handy bits of sql for pipeline use

 - stopping/restarting

Extending
---------

 - using another queueing system

 - Writing your own runnables and runnableDBs

   - what is a Runnable

   - what is a RunnableDB

   - how do they work together

   - how do they fit in with the pipeline?

   - What do I have to do to make my own Runnable(DBs)
         - easy example 
         - composite example (The Exonerate EST RunnableDB or the Gene_Builder)

Gotchas
-------




The Ensembl Analysis System
---------------------------
Michele Clamp 
The Sanger Centre
20th October 2001

Introduction
------------

What is it?  - set of perl modules designed to run a number of
different analyses over a large number of different queries.

Initially designed to run analysis for the ensembl genome database.
The framework of the modules doesn't need a database underneath and
each individual module can be used alone reading input from
flatfiles.  This can be very useful when prototyping new types of
analysis or if you only have a small number of jobs to run and a full
pipeline system is overkill.

The set of analysis modules provided are the ones used to run the
ensembl gene building system.  If you are so inclined new modules can
be written very easily (we had a new person start in ensembl knowing
no perl at all and within 2 weeks a new module was working).

Runnables
---------

At the core of the system the modules that actually do all the work
i.e. run the programs and parse the output.  These are based on an
interface

Bio::EnsEMBL::Pipeline::RunnableI

This interface has two methods that must be implemented to fit in with
the pipeline

 ->run
 ->output

Nothing else.

The run method runs the program (aftger creating thenecessary input
files), parses the output and puts the output into objects (usually
Bio::EnsEMBL::SeqFeature or Bio::EnsEMBL::FeaturePair).

The output method is used to retrieve the output data which is
returned to you as an array of objects.

The other method that needs to be written is the constructor.  There is
no defined format for a runnable constructor as each program needs
different types of input and parameters.  What is not enforced but
*strongly* encouraged is that all parameters passed to the constructor
are objects where possible.  i.e.

my $runnable = new Bio::EnsEMBL::Pipeline::Runnable::MyProg
   (-seq =>  $seq,
    -db  => 'swall');

where $seq is a Bio::Seq

and not

my $runnable = new Bio::EnsEMBL::Pipeline::Runnable::MyProg (
   -seq => $seqfile, 
   -db => 'swall');
  
where $seqfile is a file containing a sequence.

To make the system run more smoothly passing objects into and out of
runnables gives the user much more flexibility and the system as a
whole is less dependent on the underlying file system (mainly the
dreaded NFS).  Not using files means runnables can be chained together
more easily, there is no reliance on directories or files being there
and all temporary file creation is hidden from the user.

An extra win for the ensembl user is that if you are running analysis
from a database you don't have any files in the first place -
everything is patiently sitting in it's table.  As we want to pass
objects into and out of the analysis system all we have to do is use
the EnsEMBL perl interface to retrieve objects directly from the
database and pass them straight to the runnables with no intervening
files having to be created.  There is a well defined way to do this
(RunnableDB) which will be described in the next section.


Example Runnable Usage 
----------------------

my $seqio = new Bio::SeqIO(-fh => \*IN,\ -format => 'fasta');

my $seq = $seqio->next_seq;

my $run = new Bio::EnsEMBL::Pipeline::Runnable::CPG(-seq => $seq);

$run->run;

foreach my $f ($run->output) { 
  print "Output feature is " . $f->gffstring . "\n"; 
}

Now I think that is rather elegant.  All file parsing is hidden, the
code is simple, readable and all round pretty damn cool.


Writing your own runnables 
--------------------------

There are 3 steps to writing your own runnables : the constructor, the
run method and the output method.

1. Constructor

When creating your new object you need to decide what input it needs.
It will probably need a sequence of some sort, some parameters and
maybe some other things.  Try to restrict your input to objectgs or
simple data types like strings or numbers.  Keep filehandles or
filenames to an absolute minimum.  There are times when this isn't
possible - local blast databases for instance when the best way is
just to pass a file in.  Other examples are data files for programs
e.g. the HumanIso.mat file needed for genscan.  When you have to do
this try to keep all your data and binary files in the same place (and
preferably separate from all the other nonsense used in the building).
For instance we have /usr/local/ensembl/bin to store our pipeline
binaries and /usr/local/ensembl/data to store our data files.  This is
separate from the all encompassing /usr/local/bin which has 3 versions
of perl, 4 versions of netscape, old binaries going back to 1984 and
general all round cruftiness.

2. ->run

This is the most complicated method and will take up most of your
time.  It needs to take the input objects and parameters and construct
a command line (probably creating temporary input and output files).
It then needs to parse the output and put the data into objects and
store them in the output array.

There are methods that already exist to do the most common tasks like
creating sequence files and temporary files.  These are created in
/tmp which, in most cases, will be local to the machine the program is
running on.  This cuts down the use of NFS, keeps network traffic down
and uses the fastest diskIO to speed up the runtime of the program.
This may not seem important if all you are doing is running 100 blast
jobs but once you get into the millions of jobs over hundreds of
machines these things suddenly come into sharp focus.

Methods that you might want to use are :

writefile 
tempfile 
deletefile

3 ->output

This is the simplest method and should only take a minute or so.  All
it needs to do is to return the array of output features.


What if my output data won't fit into SeqFeature or FeaturePairs?  -
Write your own objects.


Chaining runnables 
------------------

As the ensembl analysis gets closer and closer to the final gene build
the modules get more and more complicated.  The modules use several of
the runnables and chain them together to do complex analyses.

For example part of the gene build consists of running blast and
running MiniGeneWise against every protein hit.  This is done in two
steps to reduce the amount of CPU used down to a realistic level.

#First of all do the blast

my $blast = new Bio::EnsEMBL::Pipeline::Runnable::Blast( 
  -seq       => $seq,
  -db        => $db, 
  -threshold => $threshold, );

$blast->run;


# Now retrieve all the features hit and group them by hid my %hid;

foreach my $f ($blast->output) { 
  if (!defined($hid{$f->hseqname})) {
    $hid{$f->hseqname} = []; 
  }

  push(@{$hid{$f->hseqname}},$f); 
}


# Now create MiniGenewise jobs for all the different proteins hit.

foreach my $hit (keys %hid) { 
  my @features = @{$hid{$hit}}; 
  my $genewise = new Bio::EnsEMBL::Pipeline::Runnable::MiniGenewise( 
    -seq      => $seq, 
    -features => @features);

  $genewise->run;

  push($self->{_output},$genewise->output);
}

#Result is genewise run in a fraction of the time it would have
#originally taken


To see the real implementation of this analysis have a look at

Bio::EnsEMBL::Pipeline::Runnable::BlastMiniGenewise.pm


RunnableDBs 
-----------

So runnables are a good way of running small analyses, most useful if
you have a few fasta files hanging around than need processing.  Once
you have more than a few sequences it makes sense to store them in a
database and access them from there.

The Bio::EnsEMBL::Pipeline::RunnableDBI interface is an easy way to
run programs on sequences that are stored in an ensembl database.  The
methods are :

 ->input_id 
 ->fetch_input 
 ->run            (inherited from RunnableI) 
 ->output         (inherited from RunnableI) 
 ->write_output


As you can see RunnableDBI inherits from Runnable so we still have the
run and output methods.  There are three extra methods which deal with
fetching data from the database and writing the output back again

->input_id
  
  This tells the RunnableDB what to fetch from the database.  For an
  ensembl database it would most probably be a contig id (something
  like AC005663.1.1.1010456).  As to what sort of thing to fetch from
  the database it depends on the RunnableDB itself.  It will know
  whether to fetch a contig or a gene or a slice of chromosome.

->fetch_input

  This method takes the input id and fetches the relevant object from
  the database ready to send it to the Runnable.

->run

  This will commonly take the input object, create a runnable with it
  and run it.  It should now become clear why we are so insistent
  about not using files for runnables.  We are fetching data from a
  database and the most convenient way of fetching that data is by
  using an object.

->output

  This will usually call the output method on the Runnable and return
  the data stored in that.

->write_output

  Just as we got our input data from a database most of the time we
  want to store the output data in a database.  This will take the
  output features from the runnable and write them back to the
  database.  For an ensembl database this will limit you to the type
  of features to write back to SeqFeature FeaturePair Clone Contig and
  Genes but this encompasses quite a lot.

Using RunnableDBs with an Ensembl database
------------------------------------------

This example takes all contigs in an ensembl database and runs
repeatmasker on them.

my $db = new Bio::EnsEMBL::DBSQL::DBAdaptor(
  -host   => 'myhost',
  -dbname => 'mydb', 
  -user   => 'pog', 
  -pass   => 'sog);

foreach my $contigid ($db->get_all_Contig_id) { 
  my $runnabledb = new Bio::EnsEMBL::Pipeline::RunnableDB::RepeatMasker(
     -db => $db,
     -input_id => $contigid);

  $runnabledb->fetch_input; 
  $runnabledb->run;
  $runnabledb->write_output;

}


Now isn't that pretty?  We don't even have to know in advance what the
contig ids are as we can get them from the database.  Also note that
*any* RunnableDB can be run like this - all we have to do is replace
the Bio::EnsEMBL::Pipeline::RunnableDB::RepeatMasker with the module
we want to run. In fact the core script that runs all of the pipeline
jobs is only slightly more complicated than this.

A very useful testing script is test_RunnableDB which is used to test
new RunnableDBs before they are allowed onto the pipeline.   It can
run any type of RunnableDB and the core of it goes as follows :



my $runnable = new Bio::EnsEMBL::Pipeline::Runnable::MyProg (
   -seq => $seqfile, 
   -db  => 'swall');

  
where $seqfile is a file containing a sequence.

To make the system run more smoothly passing objects into and out of
runnables gives the user much more flexibility and the system as a
whole is less dependent on the underlying file system (mainly the
dreaded NFS).  Not using files means runnables can be chained together
more easily, there is no reliance on directories or files being there
and all temporary file creation is hidden from the user.

An extra win for the ensembl user is that if you are running analysis
from a database you don't have any files in the first place -
everything is patiently sitting in it's table.  As we want to pass
objects into and out of the analysis system all we have to do is use
the EnsEMBL perl interface to retrieve objects directly from the
database and pass them straight to the runnables with no intervening
files having to be created.  There is a well defined way to do this
(RunnableDB) which will be described in the next section.

Building a pipeline
-------------------

To run the ensembl pipeline you will need 

    - an ensembl database.
    - some runnables and some runnableDBs
    - some sort of batch submission system.  The code is currently set
      up to either run using LSF or to run locally on a single
      processor.  I strongly recommend you get your pipeline working
      locally on a single processor before braving the wilds of LSF
      submission.  You will save hair in the long run.

Step 1)

The database schema.  There are a few extra tables to be added on to
the the core ensembl schema.  These track each job as it goes through
the system and makes it easier to pinpoint failures.  The extra
tracking tables are

 - job
 - jobstatus
 - current_status

For the pipeline itself there are 3 extra tables that define which
analyses to run on which input_ids.  They also store which ones have
already been run and the dependencies between them.  For instance
blast jobs can only be run after the sequences have been repeatmasked
so there is a rule that says don't submit any blast jobs until the
repeatmasking analysis has finished.

The extra tables are

 - InputIdAnalysis (Which analyses have been run on which input_ids)

This table gives you a current snapshot of how your pipeline is
progressing.  It doesn't tell you what is currently in the system but
only those successful jobs.  Our pipeline currently is in the
following state :

The pipeline will only run on those input_ids that appear in this
table.  Sometimes you only want to run things on a subset of sequences
and so you only put those sequence ids in the InputIdAnalysis table.

To trigger the pipeline to run over a sequence there is a dummy
analysisprocess row which has the logicname 'SubmitContig'.  Inserting
a sequences id into the InputIdAnalysis table with this analysis id
says to the pipeline = 'here is a new sequence.  Off you go'

For individual sequences there is a call to do this

 - $db->submitContig ?


 - sql and output

 - RuleGoal

This table lists the analyses that you want to run through your
pipeline.  The first column is the RuleId and the second column is the
analysisId which refers to the analysisprocess table.  The
analysisprocess tables can contain any number of analyses but they
won't necessarily be part of the pipeline.  The only analyses that get
run through the pipeline system are those that appear in the RuleGoal
table.

 - RuleCondition

Here is where we defined which analysisprocesses can be run when,
i.e. you can run things like CPG and e-PCR immediately but blast jobs
need to be run after RepeatMasker and genewise jobs need to be run
after blast jobs.

There can be multiple conditions per analysisprocess but currently
all of our analyses only have one dependency.

The RuleCondition table has two columns.  First of all there is the
RuleId column so we can refer back to the RuleGoal table so see what
analysis this condition refers to.  The second column is an analysisId
that must be fulfilled before this current analysis can start.

Example RuleGoal and RuleCondition tables


pipeConf

RuleManager

 - testing all the Runnables
 - testing all the RunnableDBs


RuleManager3.pl





